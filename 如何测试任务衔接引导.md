# 如何测试任务衔接引导优化

## 快速测试

运行测试脚本：
```bash
cd backend
python ../test_transition_guide.py
```

## 完整工作流测试

### 1. 启动后端服务
```bash
cd backend
python api.py
```

### 2. 启动前端服务
```bash
cd web
npm run dev
```

### 3. 测试流程

#### 场景1：首次进入新任务
1. 上传简历，生成优化计划
2. 观察 Guide Agent 的第一条消息
3. **预期效果**：应该看到结构化的开场引导，包含：
   - 📋 任务简介
   - 🔍 问题诊断
   - 🎯 优化目标
   - 💡 需要了解的信息

#### 场景2：跳过任务
1. 在任务进行中点击"跳过任务"按钮
2. **预期效果**：应该看到：
   ```
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   ✓ 已跳过任务 X：任务名称
   
   📋 进度：已完成 X/总数 | 已跳过 X/总数
   
   ⏭️ 接下来：任务 Y - 任务名称
   
   💡 提示：请继续对话，我会引导你完成下一个任务。
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   ```
3. 然后 Guide Agent 应该自动发送下一个任务的开场引导

#### 场景3：完成任务
1. 完成一个任务的优化并确认
2. **预期效果**：应该看到：
   ```
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   ✅ 任务 X 已完成：任务名称
   
   📋 进度：已完成 X/总数
   
   ⏭️ 接下来：任务 Y - 任务名称
      问题：XXX...
   
   💡 继续对话，我会引导你完成下一个优化。
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   ```
3. 然后 Guide Agent 应该自动发送下一个任务的开场引导

#### 场景4：完成所有任务
1. 完成最后一个任务
2. **预期效果**：应该看到：
   ```
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   ✅ 任务 X 已完成：任务名称
   
   📋 进度：已完成 X/X
   
   🎉 恭喜！所有优化任务已完成！
   
   您的简历已经过全面优化，现在可以导出使用了。
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━
   ```

## 对比测试

### 改进前的问题
- Guide Agent 等待用户先说话，没有主动引导
- 跳过/完成任务后只有简单的一句话提示
- 用户不知道下一步该做什么

### 改进后的效果
- Guide Agent 主动开场，详细说明任务背景和需要的信息
- 任务切换时有清晰的总结和引导
- 用户明确知道进度和下一步行动

## 注意事项

1. **编码问题**：Windows 环境下如果出现编码错误，确保使用 UTF-8 编码
2. **首次对话**：Guide Agent 的开场引导只在每个任务的首次对话时触发
3. **消息格式**：系统消息使用了特殊字符（✓ ✅ ⏭️ 💡 等），确保终端/浏览器支持 UTF-8

## 相关文件

- `backend/guide_agent.py` - Guide Agent 提示词优化
- `backend/orchestrator.py` - 任务切换消息优化
- `test_transition_guide.py` - 自动化测试脚本
- `TRANSITION_GUIDE_IMPLEMENTATION_SUMMARY.md` - 完整实施总结

